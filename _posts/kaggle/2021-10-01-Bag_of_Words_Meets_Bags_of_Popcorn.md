---
layout: post
title: Bag of Words Meets Bags of Popcorn ìºê¸€
categories: dev
class: post-template
comments: true
---
{% include python-table-of-contents.html %}


Bag of Words Meets Bags of PopcornÂ¶

https://www.kaggle.com/c/word2vec-nlp-tutorial


```python
!git clone https://github.com/SEONGJAE-YOO/Kaggle.git
```

    Cloning into 'Kaggle'...
    remote: Enumerating objects: 3, done.[K
    remote: Counting objects: 100% (3/3), done.[K
    remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0[K
    Unpacking objects: 100% (3/3), done.


# ê°œìš”
NLP(ìì—°ì–´ì²˜ë¦¬)ëŠ” í…ìŠ¤íŠ¸ ë¬¸ì œì— ì ‘ê·¼í•˜ê¸° ìœ„í•œ ê¸°ìˆ ì§‘í•©ì´ë‹¤. ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” IMDB ì˜í™” ë¦¬ë·°ë¥¼ ë¡œë”©í•˜ê³  ì •ì œí•˜ê³  ê°„ë‹¨í•œ BOW(Bag of Words) ëª¨ë¸ì„ ì ìš©í•˜ì—¬ ë¦¬ë·°ê°€ ì¶”ì²œì¸ì§€ ì•„ë‹Œì§€ì— ëŒ€í•œ ì •í™•ë„ë¥¼ ì˜ˆì¸¡í•œë‹¤.

# BOW(bag of words)
ê°€ì¥ ê°„ë‹¨í•˜ì§€ë§Œ íš¨ê³¼ì ì´ë¼ ë„ë¦¬ì“°ì´ëŠ” ë°©ë²•
ì¥, ë¬¸ë‹¨, ë¬¸ì¥, ì„œì‹ê³¼ ê°™ì€ ì…ë ¥ í…ìŠ¤íŠ¸ì˜ êµ¬ì¡°ë¥¼ ì œì™¸í•˜ê³  ê° ë‹¨ì–´ê°€ ì´ ë§ë­‰ì¹˜ì— ì–¼ë§ˆë‚˜ ë§ì´ ë‚˜íƒ€ë‚˜ëŠ”ì§€ë§Œ í—¤ì•„ë¦°ë‹¤.
êµ¬ì¡°ì™€ ìƒê´€ì—†ì´ ë‹¨ì–´ì˜ ì¶œí˜„íšŸìˆ˜ë§Œ ì„¸ê¸° ë•Œë¬¸ì— í…ìŠ¤íŠ¸ë¥¼ ë‹´ëŠ” ê°€ë°©(bag)ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤.
BOWëŠ” ë‹¨ì–´ì˜ ìˆœì„œê°€ ì™„ì „íˆ ë¬´ì‹œ ëœë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì˜ë¯¸ê°€ ì™„ì „íˆ ë°˜ëŒ€ì¸ ë‘ ë¬¸ì¥ì´ ìˆë‹¤ê³  í•˜ë‹¤.
it's bad, not good at all.
it's good, not bad at all.
ìœ„ ë‘ ë¬¸ì¥ì€ ì˜ë¯¸ê°€ ì „í˜€ ë°˜ëŒ€ì§€ë§Œ ì™„ì „íˆ ë™ì¼í•˜ê²Œ ë°˜í™˜ëœë‹¤.
ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ n-gramì„ ì‚¬ìš©í•˜ëŠ” ë° BOWëŠ” í•˜ë‚˜ì˜ í† í°ì„ ì‚¬ìš©í•˜ì§€ë§Œ n-gramì€ nê°œì˜ í† í°ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.

Bag-of-words model - Wikipedia


```python
from google.colab import drive

drive.mount('/content/drive/')

 
```

    Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount("/content/drive/", force_remount=True).



```python
import pandas as pd

"""
header = 0 ì€ íŒŒì¼ì˜ ì²« ë²ˆì§¸ ì¤„ì— ì—´ ì´ë¦„ì´ ìˆìŒì„ ë‚˜íƒ€ë‚´ë©° 
delimiter = \t ëŠ” í•„ë“œê°€ íƒ­ìœ¼ë¡œ êµ¬ë¶„ë˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
quoting = 3ì€ ìŒë”°ì˜´í‘œë¥¼ ë¬´ì‹œí•˜ë„ë¡ í•œë‹¤.
"""
# QUOTE_MINIMAL (0), QUOTE_ALL (1), 
# QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).

# ë ˆì´ë¸”ì¸ sentiment ê°€ ìˆëŠ” í•™ìŠµ ë°ì´í„°
train = pd.read_csv('/content/drive/MyDrive/kaggle/data/labeledTrainData.tsv', delimiter='\t', quoting=3)
# ë ˆì´ë¸”ì´ ì—†ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°
test = pd.read_csv('/content/drive/MyDrive/kaggle/data/testData.tsv', delimiter='\t', quoting=3)
train.shape
```




    (25000, 3)




```python
train.tail(3)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>sentiment</th>
      <th>review</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>24997</th>
      <td>"10905_3"</td>
      <td>0</td>
      <td>"Guy is a loser. Can't get girls, needs to bui...</td>
    </tr>
    <tr>
      <th>24998</th>
      <td>"10194_3"</td>
      <td>0</td>
      <td>"This 30 minute documentary BuÃ±uel made in the...</td>
    </tr>
    <tr>
      <th>24999</th>
      <td>"8478_8"</td>
      <td>1</td>
      <td>"I saw this movie as a child and it broke my h...</td>
    </tr>
  </tbody>
</table>
</div>




```python
test.shape
```




    (25000, 2)




```python
test.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>review</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>"12311_10"</td>
      <td>"Naturally in a film who's main themes are of ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>"8348_2"</td>
      <td>"This movie is a disaster within a disaster fi...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>"5828_4"</td>
      <td>"All in all, this is a movie for kids. We saw ...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>"7186_2"</td>
      <td>"Afraid of the Dark left me with the impressio...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>"12128_7"</td>
      <td>"A very accurate depiction of small time mob l...</td>
    </tr>
  </tbody>
</table>
</div>




```python
train.columns.values
```




    array(['id', 'sentiment', 'review'], dtype=object)




```python
test.columns.values
```




    array(['id', 'review'], dtype=object)




```python
train.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 25000 entries, 0 to 24999
    Data columns (total 3 columns):
     #   Column     Non-Null Count  Dtype 
    ---  ------     --------------  ----- 
     0   id         25000 non-null  object
     1   sentiment  25000 non-null  int64 
     2   review     25000 non-null  object
    dtypes: int64(1), object(2)
    memory usage: 586.1+ KB



```python
train.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sentiment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>25000.00000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.50000</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.50001</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.50000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>1.00000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.00000</td>
    </tr>
  </tbody>
</table>
</div>




```python
train['sentiment'].value_counts()
```




    1    12500
    0    12500
    Name: sentiment, dtype: int64




```python
train['review'][0][:700]
```




    '"With all this stuff going down at the moment with MJ i\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\'s feeling towards the press and also the obvious message of drugs are bad m\'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely lik'



# ë°ì´í„° ì •ì œ Data Cleaning and Text Preprocessing
ê¸°ê³„ê°€ í…ìŠ¤íŠ¸ë¥¼ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•´ ì¤€ë‹¤.

ì‹ í˜¸ì™€ ì†ŒìŒì„ êµ¬ë¶„í•œë‹¤. ì•„ì›ƒë¼ì´ì–´ë°ì´í„°ë¡œ ì¸í•œ ì˜¤ë²„í”¼íŒ…ì„ ë°©ì§€í•œë‹¤.

BeautifulSoup(ë·°í‹°í’€ìˆ©)ì„ í†µí•´ HTML íƒœê·¸ë¥¼ ì œê±°
ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì•ŒíŒŒë²³ ì´ì™¸ì˜ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
NLTK ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ ë¶ˆìš©ì–´(Stopword)ë¥¼ ì œê±°
ì–´ê°„ì¶”ì¶œ(ìŠ¤í…Œë° Stemming)ê³¼ ìŒì†Œí‘œê¸°ë²•(Lemmatizing)ì˜ ê°œë…ì„ ì´í•´í•˜ê³  SnowballStemmerë¥¼ í†µí•´ ì–´ê°„ì„ ì¶”ì¶œ
í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ ì´í•´í•˜ê¸°
(ì¶œì²˜ : íŠ¸ìœ„í„° í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°)

ì •ê·œí™” normalization (ì…ë‹ˆë‹¼ã…‹ã…‹ -> ì…ë‹ˆë‹¤ ã…‹ã…‹, ìƒ¤ë¦‰í•´ -> ì‚¬ë‘í•´)

í•œêµ­ì–´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¼ã…‹ã…‹ã…‹ã…‹ã…‹ -> í•œêµ­ì–´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤ ã…‹ã…‹
í† í°í™” tokenization

í•œêµ­ì–´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤ ã…‹ã…‹ -> í•œêµ­ì–´Noun, ë¥¼Josa, ì²˜ë¦¬Noun, í•˜ëŠ”Verb, ì˜ˆì‹œNoun, ì…Adjective, ë‹ˆë‹¤Eomi ã…‹ã…‹KoreanParticle
ì–´ê·¼í™” stemming (ì…ë‹ˆë‹¤ -> ì´ë‹¤)

í•œêµ­ì–´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤ ã…‹ã…‹ -> í•œêµ­ì–´Noun, ë¥¼Josa, ì²˜ë¦¬Noun, í•˜ë‹¤Verb, ì˜ˆì‹œNoun, ì´ë‹¤Adjective, ã…‹ã…‹KoreanParticle
ì–´êµ¬ ì¶”ì¶œ phrase extraction

í•œêµ­ì–´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤ ã…‹ã…‹ -> í•œêµ­ì–´, ì²˜ë¦¬, ì˜ˆì‹œ, ì²˜ë¦¬í•˜ëŠ” ì˜ˆì‹œ
Introductory Presentation: Google Slides

ë·°í‹°í’€ìˆ©ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ìš°ì„  ì„¤ì¹˜í•´ ì¤€ë‹¤.
!pip install BeautifulSoup4


```python
from bs4 import BeautifulSoup

example1 = BeautifulSoup(train['review'][0], "html5lib")
print(train['review'][0][:700])
example1.get_text()[:700]
```

    "With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely lik
    




    '"With all this stuff going down at the moment with MJ i\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\'s feeling towards the press and also the obvious message of drugs are bad m\'kay.Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyw'




```python
#ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•´ì„œ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°
import re
#ì†Œë¬¸ìì™€ ëŒ€ë¬¸ìê°€ ì•„ë‹Œ ê²ƒì€ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´í•œë‹¤
letters_only = re.sub('[^a-zA-Z]',' ',example1.get_text())
letters_only[:700]
```




    ' With all this stuff going down at the moment with MJ i ve started listening to his music  watching the odd documentary here and there  watched The Wiz and watched Moonwalker again  Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent  Moonwalker is part biography  part feature film which i remember going to see at the cinema when it was originally released  Some of it has subtle messages about MJ s feeling towards the press and also the obvious message of drugs are bad m kay Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyw'




```python
#ëª¨ë‘ ì†Œë¬¸ìë¡œ ë³€í™˜í•œë‹¤.
lower_case = letters_only.lower()
#ë¬¸ìë¡œ ë‚˜ëˆˆë‹¤ => í† í°í™”
words = lower_case.split()
print(len(words))
words[:10]
```

    437
    




    ['with',
     'all',
     'this',
     'stuff',
     'going',
     'down',
     'at',
     'the',
     'moment',
     'with']



- ë¶ˆìš©ì–´ ì œê±°(Stopword Removal)

ì¼ë°˜ì ìœ¼ë¡œ ì½”í¼ìŠ¤ì—ì„œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ëŠ” í•™ìŠµ ëª¨ë¸ë¡œì„œ í•™ìŠµì´ë‚˜ ì˜ˆì¸¡ í”„ë¡œì„¸ìŠ¤ì— ì‹¤ì œë¡œ ê¸°ì—¬í•˜ì§€ ì•Šì•„ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ì™€ êµ¬ë³„í•˜ì§€ ëª»í•œë‹¤. ì˜ˆë¥¼ë“¤ì–´ ì¡°ì‚¬, ì ‘ë¯¸ì‚¬, i, me, my, it, this, that, is, are ë“± ê³¼ ê°™ì€ ë‹¨ì–´ëŠ” ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•˜ì§€ë§Œ ì‹¤ì œ ì˜ë¯¸ë¥¼ ì°¾ëŠ”ë° í° ê¸°ì—¬ë¥¼ í•˜ì§€ ì•ŠëŠ”ë‹¤. StopwordsëŠ” "to"ë˜ëŠ” "the"ì™€ ê°™ì€ ìš©ì–´ë¥¼ í¬í•¨í•˜ë¯€ë¡œ ì‚¬ì „ ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ì œê±°í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. NLTKì—ëŠ” 153 ê°œì˜ ì˜ì–´ ë¶ˆìš©ì–´ê°€ ë¯¸ë¦¬ ì •ì˜ë˜ì–´ ìˆë‹¤. 17ê°œì˜ ì–¸ì–´ì— ëŒ€í•´ ì •ì˜ë˜ì–´ ìˆìœ¼ë©° í•œêµ­ì–´ëŠ” ì—†ë‹¤


```python
# í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìœ„í•´ ë‹¤ìŒì„ ë‹¤ìš´ë¡œë“œ í•´ì•¼í•¨
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Unzipping corpora/stopwords.zip.
    [nltk_data] Downloading package wordnet to /root/nltk_data...
    [nltk_data]   Unzipping corpora/wordnet.zip.
    




    True




```python
from nltk.corpus import stopwords
```


```python
stopwords.words('english')[:10]
```




    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're"]




```python
# stopwords ë¥¼ ì œê±°í•œ í† í°ë“¤ 
words = [w for w in words if not w in stopwords.words('english')]
print(len(words))
words[:10]
```

    219
    




    ['stuff',
     'going',
     'moment',
     'mj',
     'started',
     'listening',
     'music',
     'watching',
     'odd',
     'documentary']



# ìŠ¤í…Œë°(ì–´ê°„ì¶”ì¶œ, í˜•íƒœì†Œ ë¶„ì„)
ì¶œì²˜ : ì–´ê°„ ì¶”ì¶œ - ìœ„í‚¤ë°±ê³¼, ìš°ë¦¬ ëª¨ë‘ì˜ ë°±ê³¼ì‚¬ì „

ì–´ê°„ ì¶”ì¶œ(èªå¹¹ æŠ½å‡º, ì˜ì–´: stemming)ì€ ì–´í˜•ì´ ë³€í˜•ëœ ë‹¨ì–´ë¡œë¶€í„° ì ‘ì‚¬ ë“±ì„ ì œê±°í•˜ê³  ê·¸ ë‹¨ì–´ì˜ ì–´ê°„ì„ ë¶„ë¦¬í•´ ë‚´ëŠ” ê²ƒ
"message", "messages", "messaging" ê³¼ ê°™ì´ ë³µìˆ˜í˜•, ì§„í–‰í˜• ë“±ì˜ ë¬¸ìë¥¼ ê°™ì€ ì˜ë¯¸ì˜ ë‹¨ì–´ë¡œ ë‹¤ë£° ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤€ë‹¤.
stemming(í˜•íƒœì†Œ ë¶„ì„): ì—¬ê¸°ì—ì„œëŠ” NLTKì—ì„œ ì œê³µí•˜ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•œë‹¤. í¬í„° í˜•íƒœì†Œ ë¶„ì„ê¸°ëŠ” ë³´ìˆ˜ì ì´ê³  ë­ì»¤ìŠ¤í„° í˜•íƒœì†Œ ë¶„ì„ê¸°ëŠ” ì¢€ ë” ì ê·¹ì ì´ë‹¤. í˜•íƒœì†Œ ë¶„ì„ ê·œì¹™ì˜ ì ê·¹ì„± ë•Œë¬¸ì— ë­ì»¤ìŠ¤í„° í˜•íƒœì†Œ ë¶„ì„ê¸°ëŠ” ë” ë§ì€ ë™ìŒì´ì˜ì–´ í˜•íƒœì†Œë¥¼ ìƒì‚°í•œë‹¤


```python
# í¬í„° ìŠ¤íƒœë¨¸ì˜ ì‚¬ìš© ì˜ˆ
stemmer = nltk.stem.PorterStemmer()
print(stemmer.stem('maximum'))
print("The stemmed form of running is: {}".format(stemmer.stem("running")))
print("The stemmed form of runs is: {}".format(stemmer.stem("runs")))
print("The stemmed form of run is: {}".format(stemmer.stem("run")))
```

    maximum
    The stemmed form of running is: run
    The stemmed form of runs is: run
    The stemmed form of run is: run



```python
# ë­ì»¤ìŠ¤í„° ìŠ¤íƒœë¨¸ì˜ ì‚¬ìš© ì˜ˆ
from nltk.stem.lancaster import LancasterStemmer
lancaster_stemmer = LancasterStemmer()
print(lancaster_stemmer.stem('maximum'))
print("The stemmed form of running is: {}".format(lancaster_stemmer.stem("running")))
print("The stemmed form of runs is: {}".format(lancaster_stemmer.stem("runs")))
print("The stemmed form of run is: {}".format(lancaster_stemmer.stem("run")))
```

    maxim
    The stemmed form of running is: run
    The stemmed form of runs is: run
    The stemmed form of run is: run



```python
#ì²˜ë¦¬ ì „ ë‹¨ì–´
words[:10]
```




    ['stuff',
     'going',
     'moment',
     'mj',
     'started',
     'listening',
     'music',
     'watching',
     'odd',
     'documentary']




```python
from nltk.stem.snowball import SnowballStemmer

stemmer = SnowballStemmer('english')
words = [stemmer.stem(w) for w in words]
# ì²˜ë¦¬ í›„ ë‹¨ì–´
words[:10]
```




    ['stuff',
     'go',
     'moment',
     'mj',
     'start',
     'listen',
     'music',
     'watch',
     'odd',
     'documentari']



- Lemmatization ìŒì†Œí‘œê¸°ë²•

ì–¸ì–´í•™ì—ì„œ ìŒì†Œ í‘œê¸°ë²• (ë˜ëŠ” lemmatization)ì€ ë‹¨ì–´ì˜ ë³´ì¡° ì •ë¦¬ ë˜ëŠ” ì‚¬ì „ í˜•ì‹ì— ì˜í•´ ì‹ë³„ë˜ëŠ” ë‹¨ì¼ í•­ëª©ìœ¼ë¡œ ë¶„ì„ ë  ìˆ˜ ìˆë„ë¡ êµ´ì ˆ ëœ í˜•íƒœì˜ ë‹¨ì–´ë¥¼ ê·¸ë£¹í™”í•˜ëŠ” ê³¼ì •ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë™ìŒì´ì˜ì–´ê°€ ë¬¸ë§¥ì— ë”°ë¼ ë‹¤ë¥¸ ì˜ë¯¸ë¥¼ ê°–ëŠ”ë°

1) *ë°°*ê°€ ë§›ìˆë‹¤.

2) *ë°°*ë¥¼ íƒ€ëŠ” ê²ƒì´ ì¬ë¯¸ìˆë‹¤.

3) í‰ì†Œë³´ë‹¤ ë‘ *ë°°*ë¡œ ë§ì´ ë¨¹ì–´ì„œ *ë°°*ê°€ ì•„í”„ë‹¤.
   ìœ„ì— ìˆëŠ” 3ê°œì˜ ë¬¸ì¥ì— ìˆëŠ” "ë°°"ëŠ” ëª¨ë‘ ë‹¤ë¥¸ ì˜ë¯¸ë¥¼ ê°–ëŠ”ë‹¤.
   ë ˆë§ˆíƒ€ì´ì œì´ì…˜ì€ ì´ë•Œ ì•ë’¤ ë¬¸ë§¥ì„ ë³´ê³  ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ì‹ë³„í•˜ëŠ” ê²ƒì´ë‹¤. ì˜ì–´ì—ì„œ meetëŠ” meetingìœ¼ë¡œ ì“°ì˜€ì„ ë•Œ íšŒì˜ë¥¼ ëœ»í•˜ì§€ë§Œ meet ì¼ ë•ŒëŠ” ë§Œë‚˜ë‹¤ëŠ” ëœ»ì„ ê°–ëŠ”ë° ê·¸ ë‹¨ì–´ê°€ ëª…ì‚¬ë¡œ ì“°ì˜€ëŠ”ì§€ ë™ì‚¬ë¡œ ì“°ì˜€ëŠ”ì§€ì— ë”°ë¼ ì í•©í•œ ì˜ë¯¸ë¥¼ ê°–ë„ë¡ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ë‹¤.



```python
from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

print(wordnet_lemmatizer.lemmatize('fly'))
print(wordnet_lemmatizer.lemmatize('flies'))

words = [wordnet_lemmatizer.lemmatize(w) for w in words]
# ì²˜ë¦¬ í›„ ë‹¨ì–´
words[:10]
```

    fly
    fly
    




    ['stuff',
     'go',
     'moment',
     'mj',
     'start',
     'listen',
     'music',
     'watch',
     'odd',
     'documentari']



# ë¬¸ìì—´ ì²˜ë¦¬


```python
def review_to_words( raw_review ):
    # 1. HTML ì œê±°
    review_text = BeautifulSoup(raw_review, 'html.parser').get_text()
    # 2. ì˜ë¬¸ìê°€ ì•„ë‹Œ ë¬¸ìëŠ” ê³µë°±ìœ¼ë¡œ ë³€í™˜
    letters_only = re.sub('[^a-zA-Z]', ' ', review_text)
    # 3. ì†Œë¬¸ì ë³€í™˜
    words = letters_only.lower().split()
    # 4. íŒŒì´ì¬ì—ì„œëŠ” ë¦¬ìŠ¤íŠ¸ë³´ë‹¤ ì„¸íŠ¸ë¡œ ì°¾ëŠ”ê²Œ í›¨ì”¬ ë¹ ë¥´ë‹¤.
    # stopwords ë¥¼ ì„¸íŠ¸ë¡œ ë³€í™˜í•œë‹¤.
    stops = set(stopwords.words('english'))
    # 5. Stopwords ë¶ˆìš©ì–´ ì œê±°
    meaningful_words = [w for w in words if not w in stops]
    # 6. ì–´ê°„ì¶”ì¶œ
    stemming_words = [stemmer.stem(w) for w in meaningful_words]
    # 7. ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ê²°í•©í•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜
    return( ' '.join(stemming_words) )
```


```python
clean_review = review_to_words(train['review'][0])
clean_review
```




    'stuff go moment mj start listen music watch odd documentari watch wiz watch moonwalk mayb want get certain insight guy thought realli cool eighti mayb make mind whether guilti innoc moonwalk part biographi part featur film rememb go see cinema origin releas subtl messag mj feel toward press also obvious messag drug bad kay visual impress cours michael jackson unless remot like mj anyway go hate find bore may call mj egotist consent make movi mj fan would say made fan true realli nice actual featur film bit final start minut exclud smooth crimin sequenc joe pesci convinc psychopath power drug lord want mj dead bad beyond mj overheard plan nah joe pesci charact rant want peopl know suppli drug etc dunno mayb hate mj music lot cool thing like mj turn car robot whole speed demon sequenc also director must patienc saint came film kiddi bad sequenc usual director hate work one kid let alon whole bunch perform complex danc scene bottom line movi peopl like mj one level anoth think peopl stay away tri give wholesom messag iron mj bestest buddi movi girl michael jackson truli one talent peopl ever grace planet guilti well attent gave subject hmmm well know peopl differ behind close door know fact either extrem nice stupid guy one sickest liar hope latter'




```python
# ì²« ë²ˆì§¸ ë¦¬ë·°ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì „ì²˜ë¦¬ í•´ì¤¬ë˜ ë‚´ìš©ì„ ì „ì²´ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.
# ì „ì²´ ë¦¬ë·° ë°ì´í„° ìˆ˜ ê°€ì ¸ì˜¤ê¸°
num_reviews = train['review'].size
num_reviews
```




    25000




```python
clean_train_reviews = []

for i in range(0, num_reviews):
    clean_train_reviews.append( review_to_words(train['review'][i]))

clean_train_reviews = []
for i in range(0, num_reviews):
    if (i + 1)%5000 == 0:
        print('Review {} of {} '.format(i+1, num_reviews))
    clean_train_reviews.append(review_to_words(train['review'][i]))
    

```

    Review 5000 of 25000 
    Review 10000 of 25000 
    Review 15000 of 25000 
    Review 20000 of 25000 
    Review 25000 of 25000 



```python
from multiprocessing import Pool
import numpy as np

def _apply_df(args):
    df, func, kwargs = args
    return df.apply(func, **kwargs)

def apply_by_multiprocessing(df, func, **kwargs):
    # í‚¤ì›Œë“œ í•­ëª© ì¤‘ workers íŒŒë¼ë©”í„°ë¥¼ êº¼ëƒ„
    workers = kwargs.pop('workers')
    # ìœ„ì—ì„œ ê°€ì ¸ì˜¨ workers ìˆ˜ë¡œ í”„ë¡œì„¸ìŠ¤ í’€ì„ ì •ì˜
    pool = Pool(processes=workers)
    # ì‹¤í–‰í•  í•¨ìˆ˜ì™€ ë°ì´í„°í”„ë ˆì„ì„ ì›Œì»¤ì˜ ìˆ˜ ë§Œí¼ ë‚˜ëˆ  ì‘ì—…
    result = pool.map(_apply_df, [(d, func, kwargs)
            for d in np.array_split(df, workers)])
    pool.close()
    # ì‘ì—… ê²°ê³¼ë¥¼ í•©ì³ì„œ ë°˜í™˜
    return pd.concat(list(result))
```


```python
%time clean_train_reviews = apply_by_multiprocessing(\
    train['review'], review_to_words, workers=4)
```

    CPU times: user 457 ms, sys: 197 ms, total: 654 ms
    Wall time: 1min 5s



```python
%time clean_test_reviews = apply_by_multiprocessing(\
    test['review'], review_to_words, workers=4)
```

    CPU times: user 422 ms, sys: 199 ms, total: 622 ms
    Wall time: 1min 3s


# ì›Œë“œ í´ë¼ìš°ë“œ
ë‹¨ì–´ì˜ ë¹ˆë„ ìˆ˜ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆì„ ë•Œ ì´ìš©í•  ìˆ˜ ìˆëŠ” ì‹œê°í™” ë°©ë²•
ë‹¨ìˆœíˆ ë¹ˆë„ ìˆ˜ë¥¼ í‘œí˜„í•˜ê¸° ë³´ë‹¤ëŠ” ìƒê´€ê´€ê³„ë‚˜ ìœ ì‚¬ë„ ë“±ìœ¼ë¡œ ë°°ì¹˜í•˜ëŠ” ê²Œ ë” ì˜ë¯¸ ìˆê¸° ë•Œë¬¸ì— í° ì •ë³´ë¥¼ ì–»ê¸°ëŠ” ì–´ë µë‹¤.


```python
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
# %matplotlib inline ì„¤ì •ì„ í•´ì£¼ì–´ì•¼ì§€ë§Œ ë…¸íŠ¸ë¶ ì•ˆì— ê·¸ë˜í”„ê°€ ë””ìŠ¤í”Œë ˆì´ ëœë‹¤.
%matplotlib inline

def displayWordCloud(data = None, backgroundcolor = 'white', width=800, height=600 ):
    wordcloud = WordCloud(stopwords = STOPWORDS, 
                          background_color = backgroundcolor, 
                         width = width, height = height).generate(data)
    plt.figure(figsize = (15 , 10))
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.show()
```


```python
# í•™ìŠµ ë°ì´í„°ì˜ ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•œ ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ê·¸ë ¤ë³¸ë‹¤.
%time displayWordCloud(' '.join(clean_train_reviews))

```



![output_41_0](../../../../OneDrive/ë°”íƒ•%20í™”ë©´/blog/GitHubPageMaker-master%20(2)/GitHubPageMaker-master/img/kaggle/Bag%20of%20Words%20Meets%20Bags%20of%20Popcorn/output_41_0.png)



    CPU times: user 21.4 s, sys: 971 ms, total: 22.4 s
    Wall time: 22.4 s



```python
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•œ ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ê·¸ë ¤ë³¸ë‹¤
%time displayWordCloud(' '.join(clean_train_reviews))
```



![output_42_0](../../../../OneDrive/ë°”íƒ•%20í™”ë©´/blog/GitHubPageMaker-master%20(2)/GitHubPageMaker-master/img/kaggle/Bag%20of%20Words%20Meets%20Bags%20of%20Popcorn/output_42_0.png)



    CPU times: user 21.7 s, sys: 691 ms, total: 22.4 s
    Wall time: 22.3 s



```python
# ë‹¨ì–´ ìˆ˜
train['num_words'] = clean_train_reviews.apply(lambda x: len(str(x).split()))
# ì¤‘ë³µì„ ì œê±°í•œ ë‹¨ì–´ ìˆ˜
train['num_uniq_words'] = clean_train_reviews.apply(lambda x: len(set(str(x).split())))
```


```python
#AttributeError: 'list' object has no attribute 'apply' í•´ê²°ë°©ì•ˆ
count_words_f = lambda x: len(str(x).split())

train['num_words'] = list(map(count_words_f, clean_train_reviews))

count_uniq_words_f = lambda x: len(set(str(x).split()))

train['num_uniq_words'] = list(map(count_uniq_words_f, clean_train_reviews))
```


```python
train.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 25000 entries, 0 to 24999
    Data columns (total 5 columns):
     #   Column          Non-Null Count  Dtype 
    ---  ------          --------------  ----- 
     0   id              25000 non-null  object
     1   sentiment       25000 non-null  int64 
     2   review          25000 non-null  object
     3   num_words       25000 non-null  int64 
     4   num_uniq_words  25000 non-null  int64 
    dtypes: int64(3), object(2)
    memory usage: 976.7+ KB



```python
# ì²« ë²ˆì§¸ ë¦¬ë·°ì— 
x = clean_train_reviews[0]
x = str(x).split()
print(len(x))
x[:10]
```

    219
    




    ['stuff',
     'go',
     'moment',
     'mj',
     'start',
     'listen',
     'music',
     'watch',
     'odd',
     'documentari']




```python
import seaborn as sns

plt.rc('font', family='Malgun Gothic') #ê¸€ì ê¹¨ì§ ë°©ì§€

fig, axes = plt.subplots(ncols=2)
fig.set_size_inches(18,6)
print('ë¦¬ë·°ë³„ ë‹¨ì–´ í‰ê·  ê°’:',train['num_words'].mean())
print('ë¦¬ë·°ë³„ ë‹¨ì–´ ì¤‘ê°„ ê°’:', train['num_words'].mean())
sns.histplot(train['num_words'], bins=100, ax=axes[0])
axes[0].axvline(train['num_words'].median(), linestyle='dashed')
axes[0].set_title('ë¦¬ë·°ë³„ ë‹¨ì–´ ìˆ˜ ë¶„í¬')

print('ë¦¬ë·°ë³„ ê³ ìœ  ë‹¨ì–´ í‰ê·  ê°’:',train['num_uniq_words'].mean())
print('ë¦¬ë·°ë³„ ê³ ìœ  ë‹¨ì–´ ì¤‘ê°„ ê°’:',train['num_uniq_words'].median())
sns.histplot(train['num_uniq_words'], bins=100,color='g', ax=axes[1])
axes[1].axvline(train['num_uniq_words'].median(), linestyle='dashed')
axes[1].set_title('ë¦¬ë·°ë³„ ê³ ìœ í•œ ë‹¨ì–´ ìˆ˜ ë¶„í¬')
```

    ë¦¬ë·°ë³„ ë‹¨ì–´ í‰ê·  ê°’: 119.52356
    ë¦¬ë·°ë³„ ë‹¨ì–´ ì¤‘ê°„ ê°’: 119.52356
    ë¦¬ë·°ë³„ ê³ ìœ  ë‹¨ì–´ í‰ê·  ê°’: 94.05756
    ë¦¬ë·°ë³„ ê³ ìœ  ë‹¨ì–´ ì¤‘ê°„ ê°’: 74.0
    




    Text(0.5, 1.0, 'ë¦¬ë·°ë³„ ê³ ìœ í•œ ë‹¨ì–´ ìˆ˜ ë¶„í¬')



    findfont: Font family ['Malgun Gothic'] not found. Falling back to DejaVu Sans.
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47532 missing from current font.
      font.set_text(s, 0.0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48624 missing from current font.
      font.set_text(s, 0.0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48324 missing from current font.
      font.set_text(s, 0.0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45800 missing from current font.
      font.set_text(s, 0.0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50612 missing from current font.
      font.set_text(s, 0.0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49688 missing from current font.
      font.set_text(s, 0.0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48516 missing from current font.
      font.set_text(s, 0.0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54252 missing from current font.
      font.set_text(s, 0.0, flags=flags)
    findfont: Font family ['Malgun Gothic'] not found. Falling back to DejaVu Sans.
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47532 missing from current font.
      font.set_text(s, 0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48624 missing from current font.
      font.set_text(s, 0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48324 missing from current font.
      font.set_text(s, 0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45800 missing from current font.
      font.set_text(s, 0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50612 missing from current font.
      font.set_text(s, 0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49688 missing from current font.
      font.set_text(s, 0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48516 missing from current font.
      font.set_text(s, 0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54252 missing from current font.
      font.set_text(s, 0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44256 missing from current font.
      font.set_text(s, 0.0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50976 missing from current font.
      font.set_text(s, 0.0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54620 missing from current font.
      font.set_text(s, 0.0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44256 missing from current font.
      font.set_text(s, 0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50976 missing from current font.
      font.set_text(s, 0, flags=flags)
    /usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54620 missing from current font.
      font.set_text(s, 0, flags=flags)




![output_47_3](../../../../OneDrive/ë°”íƒ•%20í™”ë©´/blog/GitHubPageMaker-master%20(2)/GitHubPageMaker-master/img/kaggle/Bag%20of%20Words%20Meets%20Bags%20of%20Popcorn/output_47_3.png)



# ì‚¬ì´í‚·ëŸ°ì˜ CountVectorizerë¥¼ í†µí•´ í”¼ì²˜ ìƒì„±

ì •ê·œí‘œí˜„ì‹ì„ ì‚¬ìš©í•´ í† í°ì„ ì¶”ì¶œí•œë‹¤.

ëª¨ë‘ ì†Œë¬¸ìë¡œ ë³€í™˜ì‹œí‚¤ê¸° ë•Œë¬¸ì— good, Good, gOodì´ ëª¨ë‘ ê°™ì€ íŠ¹ì„±ì´ ëœë‹¤.

ì˜ë¯¸ì—†ëŠ” íŠ¹ì„±ì„ ë§ì´ ìƒì„±í•˜ê¸° ë•Œë¬¸ì— ì ì–´ë„ ë‘ ê°œì˜ ë¬¸ì„œì— ë‚˜íƒ€ë‚œ í† í°ë§Œì„ ì‚¬ìš©í•œë‹¤.

min_dfë¡œ í† í°ì´ ë‚˜íƒ€ë‚  ìµœì†Œ ë¬¸ì„œ ê°œìˆ˜ë¥¼ ì§€ì •í•  ìˆ˜ ìˆë‹¤.


```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline

# íŠœí† ë¦¬ì–¼ê³¼ ë‹¤ë¥´ê²Œ íŒŒë¼ë©”í„° ê°’ì„ ìˆ˜ì •
# íŒŒë¼ë©”í„° ê°’ë§Œ ìˆ˜ì •í•´ë„ ìºê¸€ ìŠ¤ì½”ì–´ ì°¨ì´ê°€ ë§ì´ ë‚¨
vectorizer = CountVectorizer(analyzer = 'word', 
                             tokenizer = None,
                             preprocessor = None, 
                             stop_words = None, 
                             min_df = 2, # í† í°ì´ ë‚˜íƒ€ë‚  ìµœì†Œ ë¬¸ì„œ ê°œìˆ˜
                             ngram_range=(1, 3),
                             max_features = 20000
                            )
vectorizer
```




    CountVectorizer(analyzer='word', binary=False, decode_error='strict',
                    dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
                    lowercase=True, max_df=1.0, max_features=20000, min_df=2,
                    ngram_range=(1, 3), preprocessor=None, stop_words=None,
                    strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
                    tokenizer=None, vocabulary=None)



- sklearn.pipeline.Pipeline
1. workfolwë¥¼ ìë™í™” ì‹œí‚¬ ìˆ˜ ìˆë‹¤
2. data leakage ë¬¸ì œë¥¼ ìµœì†Œí™” ì‹œí‚¬ ìˆ˜ ìˆë‹¤
3. ì„ í˜•ì  ìˆœì„œë¡œ ë°ì´í„°ë¥¼ transformí•˜ê³  ëª¨ë¸ íŠ¸ë ˆì´ë‹ ë° í‰ê°€ ê¹Œì§€ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ê°€ëŠ¥í•˜ë‹¤.



```python
# ì—¬ê¸°ì—ì„œëŠ” í•˜ë‚˜ì˜ ê³¼ì •ë§Œ ë¬¶ì–´ì£¼ì–´ pipelineì´ ë¶ˆí•„ìš” í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
# pipelineì€ feature engineeringì˜ ì—¬ëŸ¬ ê³¼ì •ì„ ë¬¶ì–´ ì¤„ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
pipeline = Pipeline([
    ('vect', vectorizer),
])
```


```python
%time train_data_features = pipeline.fit_transform(clean_train_reviews)
train_data_features
```

    CPU times: user 30.5 s, sys: 740 ms, total: 31.3 s
    Wall time: 31.2 s
    




    <25000x20000 sparse matrix of type '<class 'numpy.int64'>'
    	with 2762268 stored elements in Compressed Sparse Row format>




```python
train_data_features.shape
```




    (25000, 20000)




```python
vocab = vectorizer.get_feature_names()
print(len(vocab))
vocab[:10]
```

    20000
    




    ['aag',
     'aaron',
     'ab',
     'abandon',
     'abbey',
     'abbi',
     'abbot',
     'abbott',
     'abc',
     'abduct']




```python
#ë²¡í„°í™” ëœ í”¼ì²˜ë¥¼ í™•ì¸í•´ ë´„
import numpy as np
dist = np.sum(train_data_features, axis=0)

for tag, count in zip(vocab,dist):
    print(count,tag)
    
pd.DataFrame(dist,columns=vocab)
```

    [[26 48 22 ... 59 40 23]] aag





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>aag</th>
      <th>aaron</th>
      <th>ab</th>
      <th>abandon</th>
      <th>abbey</th>
      <th>abbi</th>
      <th>abbot</th>
      <th>abbott</th>
      <th>abc</th>
      <th>abduct</th>
      <th>abe</th>
      <th>abhay</th>
      <th>abid</th>
      <th>abigail</th>
      <th>abil</th>
      <th>abil make</th>
      <th>abl</th>
      <th>abl get</th>
      <th>abl make</th>
      <th>abl see</th>
      <th>abl watch</th>
      <th>abli</th>
      <th>aboard</th>
      <th>abomin</th>
      <th>aborigin</th>
      <th>abort</th>
      <th>abound</th>
      <th>abraham</th>
      <th>abraham lincoln</th>
      <th>abroad</th>
      <th>abrupt</th>
      <th>absenc</th>
      <th>absent</th>
      <th>absolut</th>
      <th>absolut aw</th>
      <th>absolut brilliant</th>
      <th>absolut hilari</th>
      <th>absolut horribl</th>
      <th>absolut love</th>
      <th>absolut noth</th>
      <th>...</th>
      <th>yuen</th>
      <th>yugoslavia</th>
      <th>yup</th>
      <th>yuppi</th>
      <th>yuzna</th>
      <th>yvonn</th>
      <th>zabriski</th>
      <th>zabriski point</th>
      <th>zach</th>
      <th>zack</th>
      <th>zane</th>
      <th>zani</th>
      <th>zatoichi</th>
      <th>zealand</th>
      <th>zelah</th>
      <th>zelah clark</th>
      <th>zelda</th>
      <th>zenia</th>
      <th>zero</th>
      <th>zero day</th>
      <th>zero star</th>
      <th>zeta</th>
      <th>zeta jone</th>
      <th>zhang</th>
      <th>zip</th>
      <th>zizek</th>
      <th>zodiac</th>
      <th>zodiac killer</th>
      <th>zoe</th>
      <th>zombi</th>
      <th>zombi bloodbath</th>
      <th>zombi film</th>
      <th>zombi flick</th>
      <th>zombi movi</th>
      <th>zone</th>
      <th>zoo</th>
      <th>zoom</th>
      <th>zorro</th>
      <th>zu</th>
      <th>zucker</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>26</td>
      <td>48</td>
      <td>22</td>
      <td>288</td>
      <td>24</td>
      <td>30</td>
      <td>29</td>
      <td>30</td>
      <td>125</td>
      <td>55</td>
      <td>24</td>
      <td>28</td>
      <td>28</td>
      <td>26</td>
      <td>562</td>
      <td>25</td>
      <td>1259</td>
      <td>53</td>
      <td>32</td>
      <td>54</td>
      <td>35</td>
      <td>27</td>
      <td>37</td>
      <td>83</td>
      <td>69</td>
      <td>92</td>
      <td>63</td>
      <td>93</td>
      <td>29</td>
      <td>38</td>
      <td>136</td>
      <td>118</td>
      <td>83</td>
      <td>1850</td>
      <td>29</td>
      <td>35</td>
      <td>42</td>
      <td>23</td>
      <td>93</td>
      <td>154</td>
      <td>...</td>
      <td>21</td>
      <td>28</td>
      <td>26</td>
      <td>32</td>
      <td>25</td>
      <td>25</td>
      <td>40</td>
      <td>36</td>
      <td>22</td>
      <td>21</td>
      <td>70</td>
      <td>38</td>
      <td>33</td>
      <td>47</td>
      <td>43</td>
      <td>34</td>
      <td>28</td>
      <td>31</td>
      <td>390</td>
      <td>44</td>
      <td>32</td>
      <td>38</td>
      <td>37</td>
      <td>37</td>
      <td>23</td>
      <td>85</td>
      <td>45</td>
      <td>26</td>
      <td>27</td>
      <td>1331</td>
      <td>23</td>
      <td>52</td>
      <td>37</td>
      <td>89</td>
      <td>161</td>
      <td>31</td>
      <td>71</td>
      <td>59</td>
      <td>40</td>
      <td>23</td>
    </tr>
  </tbody>
</table>
<p>1 rows Ã— 20000 columns</p>
</div>




```python
pd.DataFrame(train_data_features[:10].toarray(), columns=vocab).head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>aag</th>
      <th>aaron</th>
      <th>ab</th>
      <th>abandon</th>
      <th>abbey</th>
      <th>abbi</th>
      <th>abbot</th>
      <th>abbott</th>
      <th>abc</th>
      <th>abduct</th>
      <th>abe</th>
      <th>abhay</th>
      <th>abid</th>
      <th>abigail</th>
      <th>abil</th>
      <th>abil make</th>
      <th>abl</th>
      <th>abl get</th>
      <th>abl make</th>
      <th>abl see</th>
      <th>abl watch</th>
      <th>abli</th>
      <th>aboard</th>
      <th>abomin</th>
      <th>aborigin</th>
      <th>abort</th>
      <th>abound</th>
      <th>abraham</th>
      <th>abraham lincoln</th>
      <th>abroad</th>
      <th>abrupt</th>
      <th>absenc</th>
      <th>absent</th>
      <th>absolut</th>
      <th>absolut aw</th>
      <th>absolut brilliant</th>
      <th>absolut hilari</th>
      <th>absolut horribl</th>
      <th>absolut love</th>
      <th>absolut noth</th>
      <th>...</th>
      <th>yuen</th>
      <th>yugoslavia</th>
      <th>yup</th>
      <th>yuppi</th>
      <th>yuzna</th>
      <th>yvonn</th>
      <th>zabriski</th>
      <th>zabriski point</th>
      <th>zach</th>
      <th>zack</th>
      <th>zane</th>
      <th>zani</th>
      <th>zatoichi</th>
      <th>zealand</th>
      <th>zelah</th>
      <th>zelah clark</th>
      <th>zelda</th>
      <th>zenia</th>
      <th>zero</th>
      <th>zero day</th>
      <th>zero star</th>
      <th>zeta</th>
      <th>zeta jone</th>
      <th>zhang</th>
      <th>zip</th>
      <th>zizek</th>
      <th>zodiac</th>
      <th>zodiac killer</th>
      <th>zoe</th>
      <th>zombi</th>
      <th>zombi bloodbath</th>
      <th>zombi film</th>
      <th>zombi flick</th>
      <th>zombi movi</th>
      <th>zone</th>
      <th>zoo</th>
      <th>zoom</th>
      <th>zorro</th>
      <th>zu</th>
      <th>zucker</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 20000 columns</p>
</div>



# ëœë¤ í¬ë ˆìŠ¤íŠ¸
ëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ ê°€ì¥ í•µì‹¬ì ì¸ íŠ¹ì§•ì€ ì„ì˜ì„±(randomness)ì— ì˜í•´ ì„œë¡œ ì¡°ê¸ˆì”© ë‹¤ë¥¸ íŠ¹ì„±ì„ ê°–ëŠ” íŠ¸ë¦¬ë“¤ë¡œ êµ¬ì„±ëœë‹¤ëŠ” ì ì´ë‹¤. ì´ íŠ¹ì§•ì€ ê° íŠ¸ë¦¬ë“¤ì˜ ì˜ˆì¸¡(prediction)ë“¤ì´ ë¹„ìƒê´€í™”(decorrelation) ë˜ê²Œí•˜ë©°, ê²°ê³¼ì ìœ¼ë¡œ ì¼ë°˜í™”(generalization) ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤. ë˜í•œ, ì„ì˜í™”(randomization)ëŠ” í¬ë ˆìŠ¤íŠ¸ê°€ ë…¸ì´ì¦ˆê°€ í¬í•¨ëœ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ê°•í•˜ê²Œ ë§Œë“¤ì–´ ì¤€ë‹¤.


```python
from sklearn.ensemble import RandomForestClassifier

# ëœë¤í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ê¸°ë¥¼ ì‚¬ìš©
forest = RandomForestClassifier(
    n_estimators = 100, n_jobs = -1, random_state=2018)
forest
```




    RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                           criterion='gini', max_depth=None, max_features='auto',
                           max_leaf_nodes=None, max_samples=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=100,
                           n_jobs=-1, oob_score=False, random_state=2018, verbose=0,
                           warm_start=False)




```python
%time forest = forest.fit(train_data_features, train['sentiment'])
```

    CPU times: user 1min 25s, sys: 164 ms, total: 1min 25s
    Wall time: 44 s



```python
from sklearn.model_selection import cross_val_score
%time score = np.mean(cross_val_score(\
    forest, train_data_features, \
    train['sentiment'], cv=10, scoring='roc_auc'))
score
```

    CPU times: user 13.6 s, sys: 1.54 s, total: 15.2 s
    Wall time: 6min 26s
    




    0.92761104



# ì˜ˆì¸¡


```python
# ìœ„ì—ì„œ ì •ì œí•´ì¤€ ë¦¬ë·°ì˜ ì²« ë²ˆì§¸ ë°ì´í„°ë¥¼ í™•ì¸
clean_test_reviews[0]
```




    'natur film main theme mortal nostalgia loss innoc perhap surpris rate high older viewer younger one howev craftsmanship complet film anyon enjoy pace steadi constant charact full engag relationship interact natur show need flood tear show emot scream show fear shout show disput violenc show anger natur joyc short stori lend film readi made structur perfect polish diamond small chang huston make inclus poem fit neat truli masterpiec tact subtleti overwhelm beauti'




```python
# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë²¡í„°í™” í•¨
%time test_data_features = pipeline.transform(clean_test_reviews)
test_data_features = test_data_features.toarray()
```

    CPU times: user 8.12 s, sys: 68.9 ms, total: 8.19 s
    Wall time: 8.2 s



```python
test_data_features
```




    array([[0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           ...,
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0],
           [0, 0, 0, ..., 0, 0, 0]])




```python
# ë²¡í„°í™” ëœ ë‹¨ì–´ë¡œ ìˆ«ìê°€ ë¬¸ì„œì—ì„œ ë“±ì¥í•˜ëŠ” íšŸìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤
test_data_features[5][:100]
```




    array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])




```python
# ë²¡í„°í™” í•˜ë©° ë§Œë“  ì‚¬ì „ì—ì„œ í•´ë‹¹ ë‹¨ì–´ê°€ ë¬´ì—‡ì¸ì§€ ì°¾ì•„ë³¼ ìˆ˜ ìˆë‹¤.
# vocab = vectorizer.get_feature_names()
vocab[8], vocab[2558], vocab[2559], vocab[2560]
```




    ('abc', 'charact person', 'charact play', 'charact plot')




```python
result = forest.predict(test_data_features)
result[:10]
```




    array([1, 0, 0, 1, 1, 1, 0, 1, 0, 0])



# ìºê¸€ ì œì¶œì„ ìœ„í•´ ì˜ˆì¸¡ê²°ê³¼ ì €ì¥


```python
# ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ ë°ì´í„°í”„ë ˆì„ì— ë‹´ì•„ ì¤€ë‹¤.
output = pd.DataFrame(data={'id':test['id'], 'sentiment':result})
output.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>sentiment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>"12311_10"</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>"8348_2"</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>"5828_4"</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>"7186_2"</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>"12128_7"</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




```python
output.to_csv('/content/drive/MyDrive/kaggle/data/tutorial_1_BOW_{0:.5f}.csv'.format(score), index=False, quoting=3)
```


```python
output_sentiment = output['sentiment'].value_counts()
print(np.abs(output_sentiment[0] - output_sentiment[1]))
output_sentiment
```

    108
    




    0    12554
    1    12446
    Name: sentiment, dtype: int64



# Train, Testì˜ ê°ì •ë¶„ë¥˜ ê²°ê³¼ ê°’ ë¹„êµ



```python
fig, axes = plt.subplots(ncols=2)
fig.set_size_inches(12,5)
sns.countplot(train['sentiment'], ax=axes[0])
sns.countplot(output['sentiment'], ax=axes[1])
```

    /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
      FutureWarning
    /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
      FutureWarning
    




    <matplotlib.axes._subplots.AxesSubplot at 0x7f8dc895f2d0>





![output_73_2](../../../../OneDrive/ë°”íƒ•%20í™”ë©´/blog/GitHubPageMaker-master%20(2)/GitHubPageMaker-master/img/kaggle/Bag%20of%20Words%20Meets%20Bags%20of%20Popcorn/output_73_2.png)



# ìºê¸€ ì œì¶œ ê²°ê³¼


```python
# íŒŒë¼ë©”í„°ë¥¼ ì¡°ì •í•´ ê°€ë©° ì ìˆ˜ë¥¼ ì¡°ê¸ˆì”© ì˜¬ë ¤ë³¸ë‹¤.

# uni-gram ì‚¬ìš© ì‹œ ìºê¸€ ì ìˆ˜ 0.84476
print(436/578)
# tri-gram ì‚¬ìš© ì‹œ ìºê¸€ ì ìˆ˜ 0.84608
print(388/578)
# ì–´ê°„ì¶”ì¶œ í›„ ìºê¸€ ì ìˆ˜ 0.84780
print(339/578)
# ëœë¤í¬ë ˆìŠ¤íŠ¸ì˜ max_depth = 5 ë¡œ ì§€ì •í•˜ê³ 
# CountVectorizerì˜ tokenizer=nltk.word_tokenize ë¥¼ ì§€ì • í›„ ìºê¸€ ì ìˆ˜ 0.81460
print(546/578)
# ëœë¤í¬ë ˆìŠ¤íŠ¸ì˜ max_depth = 5 ëŠ” ë‹¤ì‹œ Noneìœ¼ë¡œ ë³€ê²½
# CountVectorizer max_features = 10000ê°œë¡œ ë³€ê²½ í›„ ìºê¸€ ì ìˆ˜ 0.85272
print(321/578)
# CountVectorizerì˜ tokenizer=nltk.word_tokenize ë¥¼ ì§€ì • í›„ ìºê¸€ ì ìˆ˜ 0.85044
print(326/578)
# CountVectorizer max_features = 10000ê°œë¡œ ë³€ê²½ í›„ ìºê¸€ ì ìˆ˜ 0.85612
print(305/578)
# 0.85884
print(296/578)

print(310/578)
```

    0.754325259515571
    0.671280276816609
    0.5865051903114187
    0.9446366782006921
    0.5553633217993079
    0.5640138408304498
    0.527681660899654
    0.5121107266435986
    0.5363321799307958


# labeledTrainData.tsv, testData.tsv, unlabeledTrainData.tsv ë°ì´í„°ë¥¼ ë°‘ì— ë§í¬ë¥¼ í´ë¦­í•˜ë©´ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤. íŒŒì¼ì´ í¬ë‹¤ë³´ë‹ˆ ê¹ƒí—ˆë¸Œì— ì˜¬ë ¤ì§€ì§€ ì•Šì•„ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§í¬ ê³µìœ í•©ë‹ˆë‹¤

[https://drive.google.com/drive/folders/1eVFhPNhAKnJLaruEh87XJ1P8nBKLhMGG?usp=sharing](https://drive.google.com/drive/folders/1eVFhPNhAKnJLaruEh87XJ1P8nBKLhMGG?usp=sharing)
  